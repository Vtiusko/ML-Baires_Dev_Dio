{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48cd83fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from   torchvision import transforms, datasets\n",
    "import torchvision\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from   torch import nn, optim\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from   time import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e662461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixando as imagens e convertendo para tensor\n",
    "# Definindo a transformação de imagem para o modelo (Tensor)\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# =====================================================================\n",
    "# =====================================================================\n",
    "trainset = datasets.MNIST( # Carregando o conjunto de dados MNIST\n",
    "    './MNIST_data/',       # Parte do treino do dataset\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset,       # Criando o DataLoader para o conjunto de treino\n",
    "    batch_size=64,  # Cria um buffer para pegar os dados por partes\n",
    "    shuffle=True)\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# =====================================================================\n",
    "valset = datasets.MNIST( # Carrega parte da validação do dataset\n",
    "    './MNIST_data/',\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transform)\n",
    "\n",
    "valloader = torch.utils.data.DataLoader(\n",
    "    valset,    # Cria um buffer para pegar os dados por partes\n",
    "    batch_size=64,\n",
    "    shuffle=False)\n",
    "\n",
    "# =====================================================================\n",
    "# =====================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cab7d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1740885f320>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Carregando uma imagem do treinamento\n",
    "\n",
    "dataiter = iter(trainloader)\n",
    "imagens, labels = next(dataiter)\n",
    "\n",
    "plt.imshow(imagens[0].numpy().squeeze(), cmap='gray_r')  # Mostra a imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7949175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "print(imagens[0].shape) # Para verificar as dimensões do tensor de cada imagem\n",
    "print(labels[0].shape)  # Para verificar as dimensões do label de cada imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4869f238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copiar as camadas da rede InceptionV3\n",
    "class Modelo(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Modelo, self).__init__()  # Inicializa a classe pai\n",
    "        self.linear1 = nn. Linear(28*28, 128) # Camada de entrada, 784 neuronios que se ligan a 128\n",
    "        self.linear2 = nn. Linear(128, 64) # Camada interna 1, 128 neuronios que se ligan a 64\n",
    "        self.linear3 = nn.Linear(64, 10) # Camada interna 2, 64 neuronios que se ligam a 10\n",
    "# para a camada de saida nao e necessario definir nada pois so precisamos pegar o output da camada interna 2\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.linear1(X)) # Funcão de ativacão da camada de entrada para a camada interna 1\n",
    "        X = F.relu(self.linear2(X)) # Funcao de ativacao da camada interna 1 para a camada interna 2\n",
    "        X = self.linear3(X) # Função de ativação da camada interna 2 para a camada de saida, nesse caso f(x) = x\n",
    "        return F.log_softmax(X, dim=1) # Dados utilizados para calcular a perda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb68b638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementar a parte de otimização da rede\n",
    "# onde é a primeira vista a parte de atualização dos pesos\n",
    "# para isso devemos utilizar um \"otimizador\"\n",
    "\n",
    "def treinar_modelo(modelo, trainloader, device):\n",
    "    # define a política de atualização dos pesos e da bias\n",
    "    otimizador = optim.SGD(modelo.parameters(), lr=0.01, momentum=0.5)\n",
    "    inicio = time() # Time para saber quanto tempo levou o treinamento \n",
    "\n",
    "    criterio = nn.NLLLoss() # Define o critério para calcular a perda\n",
    "    EPOCHS = 30 # Define o número de épocas para o treinamento\n",
    "    modelo.train() # Ativa o modo de treinamento do modelo\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        perda_acumulada = 0 # inicia a contagem da perda acumulada\n",
    "\n",
    "        for imagens, etiquetas in trainloader:\n",
    "            imagens = imagens.view(imagens.shape[0], -1) # Redimensiona as imagens para \"Vetores\" de 28*28 casas para ficarem compatíveis com a entrada da rede\n",
    "            otimizador.zero_grad() # Zera os gradientes do otimizador por conta do ciclo anterior\n",
    "\n",
    "            output = modelo(imagens.to(device)) # Passa as imagens pelo modelo\n",
    "            perda_instantanea = criterio(output, etiquetas.to(device)) # Calcula a perda da época em questão\n",
    "            perda_instantanea.backward() # Backpropagation à partir da perda\n",
    "\n",
    "            otimizador.step() # Atualiza os pesos do modelo e as bias\n",
    "            perda_acumulada += perda_instantanea.item() # Atualização da perda acumulada\n",
    "\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS}, Perda: {perda_acumulada/len(trainloader)}\", end='\\n\\n')\n",
    "    print(f\"Tempo total de treinamento em minutos: {(time() - inicio) / 60} minutos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69207e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementando o modelo de validação\n",
    "# Vai nos mostrar a precisão do modelo (Acurácia)\n",
    "def validacao(modelo, valloader, device):\n",
    "    conta_corretas, conta_todas = 0, 0\n",
    "\n",
    "    for imagens,etiquetas in valloader:\n",
    "        for i in range(len(etiquetas)):\n",
    "            img = imagens[i].view(1, 784)\n",
    "\n",
    "            # Desativar o autograd para acelerar a validacao. Grafos computacionais dinâmicos tem um custo alto de processamento\n",
    "            with torch.no_grad():\n",
    "                logps = modelo(img.to(device)) # Output do modelo em escala logaritmica\n",
    "\n",
    "            ps = torch.exp(logps) # Converte output para escala normal(lembrando que é um tensor)\n",
    "            probab = list(ps.cpu().numpy()[0])\n",
    "            etiqueta_pred = probab.index(max(probab)) # Converte o tensor em um número, no caso, o número que o modelo previu como correto\n",
    "            etiqueta_certa = etiquetas.numpy()[i]\n",
    "            \n",
    "            if(etiqueta_certa == etiqueta_pred): # Compara a previsão com o valor correto\n",
    "                conta_corretas += 1\n",
    "            conta_todas += 1\n",
    "\n",
    "    print(\"Total de imagens testadas =\", conta_todas)\n",
    "    print(\"\\nPrecisão do modelo = {}%\".format(conta_corretas*100/conta_todas))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b3b8b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Modelo(\n",
       "  (linear1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (linear3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verificando se temos uma GPU disponível para treinamento\n",
    "modelo = Modelo()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "modelo.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265bf164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Perda: 1.1310881617099746\n",
      "\n",
      "Epoch 2/30, Perda: 0.37968996693012813\n",
      "\n",
      "Epoch 3/30, Perda: 0.3133257011106528\n",
      "\n",
      "Epoch 4/30, Perda: 0.2762480244072261\n",
      "\n",
      "Epoch 5/30, Perda: 0.24591628469224933\n",
      "\n",
      "Epoch 6/30, Perda: 0.219185466419405\n",
      "\n",
      "Epoch 7/30, Perda: 0.19676710673963338\n",
      "\n",
      "Epoch 8/30, Perda: 0.17802948775147198\n",
      "\n",
      "Epoch 9/30, Perda: 0.16179067264996103\n",
      "\n",
      "Epoch 10/30, Perda: 0.14814073337055345\n",
      "\n",
      "Epoch 11/30, Perda: 0.13649728345368972\n",
      "\n",
      "Epoch 12/30, Perda: 0.12649967342511867\n",
      "\n",
      "Epoch 13/30, Perda: 0.1178989146125199\n",
      "\n",
      "Epoch 14/30, Perda: 0.11046333207901735\n",
      "\n",
      "Epoch 15/30, Perda: 0.1030012609939506\n",
      "\n",
      "Epoch 16/30, Perda: 0.09664813100274947\n",
      "\n",
      "Epoch 17/30, Perda: 0.0908314059399116\n",
      "\n",
      "Epoch 18/30, Perda: 0.08577188144652431\n",
      "\n",
      "Epoch 19/30, Perda: 0.08136602376909383\n",
      "\n",
      "Epoch 20/30, Perda: 0.0767959913632064\n",
      "\n",
      "Epoch 21/30, Perda: 0.07280345661029505\n",
      "\n",
      "Epoch 22/30, Perda: 0.06895138304541583\n",
      "\n",
      "Epoch 23/30, Perda: 0.06577232576060746\n",
      "\n",
      "Epoch 24/30, Perda: 0.06231128166951954\n",
      "\n",
      "Epoch 25/30, Perda: 0.05964452837472722\n",
      "\n",
      "Epoch 26/30, Perda: 0.056050863951496256\n",
      "\n",
      "Epoch 27/30, Perda: 0.054150387657675214\n",
      "\n",
      "Epoch 28/30, Perda: 0.05110974709363356\n",
      "\n",
      "Epoch 29/30, Perda: 0.04883739652622467\n",
      "\n",
      "Epoch 30/30, Perda: 0.046629648051733\n",
      "\n",
      "Tempo total de treinamento em minutos: 2.1737381259600324 minutos\n",
      "Total de imagens testadas = 10000\n",
      "\n",
      "Precisão do modelo = 97.76%\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mO Kernel deu pane ao executar o código na célula atual ou em uma célula anterior. \n",
      "\u001b[1;31mAnalise o código nas células para identificar uma possível causa da pane. \n",
      "\u001b[1;31mClique <a href='https://aka.ms/vscodeJupyterKernelCrash'>aqui</a> para obter mais informações. \n",
      "\u001b[1;31mConsulte Jupyter <a href='command:jupyter.viewOutput'>log</a> para obter mais detalhes."
     ]
    }
   ],
   "source": [
    "# Chamando o modelo para executar nosso treinamento e validação\n",
    "treinar_modelo(modelo, trainloader, device)\n",
    "validacao(modelo, valloader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
